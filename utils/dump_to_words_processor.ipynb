{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurdish Wikipedia dumps to clean concated words processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "* `os` for interacting with os files\n",
    "* `sys` to print progress\n",
    "* `re` regex library\n",
    "* `bs4` or Beautiful Soup for parsing xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define few important paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_out_path = os.path.join('..', 'wikipedia', 'kurmanji', 'output', 'AA')\n",
    "output_file = os.path.join('..', 'wikipedia', 'kurmanji', 'concated', 'text8ku.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful variables\n",
    "- vocab: to keep track of the characters in the documents\n",
    "- cnt: to count the processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might need the chars for a future project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', 'char_vocab.txt'), 'r') as f:\n",
    "    lines = f.readlines()\n",
    "chars = [x.split()[1] for x in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precompiled regex patterns to clean up the data and produce one big [text8](http://mattmahoney.net/dc/textdata.html) style file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "brackets = re.compile('[\\(\\)]')\n",
    "spaces = re.compile('\\s+')\n",
    "non_vocab = re.compile('[^{}]'.format(''.join(chars)))\n",
    "# non_char = re.compile('[0-9:/\";`´°.,\\-%\\'²ı!#|\\[\\]_’*]') # List got too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pass through all documents to parse xml, clean unwanted characters, and write them to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs 23915"
     ]
    }
   ],
   "source": [
    "for wiki_extracted_partition_file in os.listdir(wiki_out_path):\n",
    "    with open(os.path.join(wiki_out_path, wiki_extracted_partition_file), 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "    all_docs = soup.findAll('doc')\n",
    "    for doc in all_docs:\n",
    "        page = doc.get_text()\n",
    "        page = re.sub(non_vocab, ' ', page)\n",
    "        page = re.sub(spaces, ' ', page)\n",
    "        vocab.update(page)\n",
    "        with open(output_file, 'a+') as f:\n",
    "            f.write(page)\n",
    "            f.write(' ')\n",
    "        cnt += 1\n",
    "        sys.stdout.write('\\rdocs %d' % cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky part and I had to hand pick the chars and write them to a file. This is a verified list from the [Wikipedia Kurmanji Alphabets](https://en.wikipedia.org/wiki/Kurdish_alphabets) page. The process was a bit messy so I only left the writing part here. If other chars are needed the section bellow will be updated. I have commented it out so that it won't be run by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join('..', 'char_vocab.txt'), 'a+') as f:\n",
    "#     for index, character in enumerate(sorted(chars)):\n",
    "#         f.write('{} {}\\n'.format(index, character))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This method of removing non-vocab characters will leave many incomplete words. For example if a word contains a Turkish letter only that letter will be removed. For now my solution is discard words that occur only once in my dataset when I use it but I can develop a different regex to remove the surrounding characters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
